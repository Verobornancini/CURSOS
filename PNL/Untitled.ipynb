{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:darkblue\"> Lematización o Stemming </span>\n",
    "* Una buena técnica para obtener la información relevante de un texto consiste en eliminar los elementos que puedan ser irrelevantes, y resaltar más lo que los textos tienen en común que sus diferencias.  \n",
    "\n",
    "## 1) Tokenización\n",
    "* Vamos a eliminar las palabras que tienen poco interés. \n",
    "* El primer paso es delimitar las palabras del texto, y convertir esas palabras en elementos de una lista (tokenizar). \n",
    "* 1) Descargar la librería spacy: !pip install -U spacy \n",
    "* 2) Descargar el modelo del español, en la terminal colocar: python -m spacy download es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from wordcloud import WordCloud\n",
    "import collections\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abro una nueva ventana que me parmite hacer la instalación de todos los paquetes que necesite de la librería nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizar\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "text = 'Soy un texto. Normalmente soy más largo y más grande. Que no te engañe mi tamaño.'\n",
    "doc = nlp(text) # Crea un objeto de spacy tipo nlp\n",
    "tokens = [t.orth_ for t in doc] # Crea una lista con las palabras del texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Limpieza del texto\n",
    "* Vamos a eliminar de esa lista las palabras muy comunes o poco informativas, desde el punto de vista léxico, tales como conjunciones (y, o, ni, que), preposiciones (a, en, para, por, entre otras) y verbos muy comunes (ser, ir, y otros más).\n",
    "* Palabras poco representativas (stopwords).\n",
    "* Obtendremos una lista de palabras que realmente podrían ser pistas sobre el tópico del texto, o que nos permitan clasificarlo más fácilmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza\n",
    "lexical_tokens = [t.orth_ for t in doc if not t.is_punct | t.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Normalizar el texto\n",
    "* Nuestro tokenizador reconoce formas como caminar, Caminar y CAMINAR como formas distintas, la normalización nos permite unificar estos datos en uno solo.\n",
    "* Aprovecharemos este paso para descartar palabras muy cortas (menores a 4 caracteres) para filtrar aún más nuestros tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar y Remover los signos de puntuación\n",
    "words = [t.lower() for t in lexical_tokens if len(t) > 3 and t.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ahora podemos poner todo junto en una función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# tokenize, clean and normalize\n",
    "def normalize(text):\n",
    "    doc = nlp(text)\n",
    "    words = [t.orth_ for t in doc if not t.is_punct | t.is_stop] \n",
    "    lexical_tokens = [t.lower() for t in words if len(t) > 3 and t.isalpha()]\n",
    "    return lexical_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_spanish = normalize(your_spanish_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-ede38cc36d29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Remove duplicates, separate by punctuation, and use delimiters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlexical_tokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Set the stopwords\n",
    "stopwords = set(line.strip() for line in open('todos_los_textos.csv', 'r',encoding=\"utf8\"))\n",
    "#stopwords = stopwords.union(set(['mr','mrs','one','two','said']))\n",
    "\n",
    "# Initializing a dictionary, for each word in the file adds a word  and if it exists increments the counter\n",
    "wordcount = {}\n",
    "\n",
    "# Remove duplicates, separate by punctuation, and use delimiters\n",
    "for word in lexical_tokens.lower().split():\n",
    "    word = word.replace(\".\",\"\")\n",
    "    word = word.replace(\",\",\"\")\n",
    "    word = word.replace(\":\",\"\")\n",
    "    word = word.replace(\"\\\"\",\"\")\n",
    "    word = word.replace(\"!\",\"\")\n",
    "    word = word.replace(\"â€œ\",\"\")\n",
    "    word = word.replace(\"â€˜\",\"\")\n",
    "    word = word.replace(\"*\",\"\")\n",
    "    if word not in stopwords:\n",
    "        if word not in wordcount:\n",
    "            wordcount[word] = 1\n",
    "        else:\n",
    "            wordcount[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the most used words\n",
    "n_print = int(10)\n",
    "print(\"\\nLas {} palabras más usadas son:\\n\".format(n_print))\n",
    "\n",
    "word_counter = collections.Counter(wordcount)\n",
    "for word, count in word_counter.most_common(n_print):\n",
    "    print(word, \": \", count)\n",
    "\n",
    "# Create a dataframe with the most used words\n",
    "list_spanish = word_counter.most_common(n_print)\n",
    "frequency = pd.DataFrame(list_spanish, columns = ['Word', 'Count'])\n",
    "\n",
    "# Gráfico de barras\n",
    "frequency.plot.bar(x='Word',y='Count', color='darkslateblue')\n",
    "plt.savefig('palabras_mas_usadas.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
